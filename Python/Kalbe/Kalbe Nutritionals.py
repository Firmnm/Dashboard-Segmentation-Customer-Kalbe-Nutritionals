# -*- coding: utf-8 -*-
"""Kalbe Nutritionals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RqT7fBXkVSiAifqC7BMTIsrXkRod92hN

# **Kalbe Nutritionals Project Based Internship**

IMPORT LIBRARY
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

import warnings
warnings.filterwarnings('ignore')

"""Convert CSV Files to Dataframe"""

df_customer = pd.read_csv('Customer.csv', delimiter= ';')
df_product = pd.read_csv('Product.csv', delimiter= ';')
df_store = pd.read_csv('Store.csv', delimiter= ';')
df_transaction = pd.read_csv('Transaction.csv', delimiter= ';')

"""Melihat ukuran data"""

df_customer.shape, df_product.shape, df_store.shape, df_transaction.shape

df_customer.head()

df_customer.dtypes

df_product.head()

df_product.dtypes

df_store.head()

df_store.dtypes

df_transaction.head()

df_transaction.dtypes

"""Data cleansing df_customer"""

df_customer['Income'] = df_customer['Income'].replace('[,]','.',regex=True).astype('float')

df_customer.head()

"""Data cleansing df_Store"""

df_store['Latitude'] = df_store['Latitude'].replace('[,]','.',regex=True).astype('float')
df_store['Longitude'] = df_store['Longitude'].replace('[,]','.',regex=True).astype('float')

"""Data cleansing df_transaction"""

df_transaction['Date'] = pd.to_datetime(df_transaction['Date'])

# Memeriksa apakah ada nilai yang hilang di setiap kolom DataFrame
missing_values1 = df_customer.isnull().sum()
missing_values2 = df_product.isnull().sum()
missing_values3 = df_store.isnull().sum()
missing_values4 = df_transaction.isnull().sum()

# Menampilkan kolom-kolom dengan nilai yang hilang (jika ada)
print('Customer', missing_values1)
print('\nProduct', missing_values2)
print('\nStore', missing_values3)
print('\nTransaction', missing_values4)

# Mengatasi Missing Value Customer (Marital Status)
df_customer['Marital Status'].fillna('No Status', inplace=True)
df_customer.isnull().sum()

# Mengecek data duplikat
duplicate_rows1 = df_customer[df_customer.duplicated()]
duplicate_rows2 = df_product[df_product.duplicated()]
duplicate_rows3 = df_store[df_store.duplicated()]
duplicate_rows4 = df_transaction[df_transaction.duplicated()]

# Menampilkan baris data yang duplikat
print("Data Duplikat Customer:", duplicate_rows1)
print("\nData Duplikat Product:", duplicate_rows2)
print("\nData Duplikat Store:", duplicate_rows2)
print("\nData Duplikat Transaction:", duplicate_rows2)

df_transaction['TransactionID'].value_counts()

df_transaction [df_transaction['TransactionID'] == 'TR71313' ]

"""Data Merge"""

df_merge = pd.merge(df_transaction, df_customer, on='CustomerID')
df_merge = pd.merge(df_merge, df_product.drop(columns=['Price']), on='ProductID')
df_merge = pd.merge(df_merge, df_store, on='StoreID')

df_merge.head()

df_merge.info()

df_merge.duplicated().sum()

"""Machine Learning Regression (Time Series)"""

df_regresi = df_merge.groupby(['Date']).agg({
    'Qty' : 'sum'
}).reset_index()

df_regresi

"""Analisis"""

# Decomposition Analysis (Trend, Seasonal, Residual)
decomposed = seasonal_decompose(df_regresi.set_index('Date'))

plt.figure(figsize=(10, 10))

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title("Trend")

plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonal')

plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residual')

plt.tight_layout()
plt.show()

"""Uji Stasioner"""

from statsmodels.tsa.stattools import adfuller

# Uji ADF pada kolom 'Qty' dari DataFrame 'df_regression'
result = adfuller(df_regresi['Qty'])

# Menampilkan hasil uji ADF
print('Augmented Dickey-Fuller Test Results:')
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

# Menyimpulkan hasil uji berdasarkan p-value
if result[1] <= 0.05:
    print('\nHasil uji menunjukkan bahwa data adalah stasioner (Reject H0)')
else:
    print('\nHasil uji menunjukkan bahwa data bukan stasioner (Fail to Reject H0)')

# Memisahkan Data
cut_off = round(df_regresi.shape[0] * 0.8)

df_train = df_regresi.loc[:cut_off]
df_test = df_regresi.loc[cut_off:].reset_index(drop=True)

df_train.shape, df_test.shape

df_train

df_test

plt.figure(figsize=(20, 5))

sns.lineplot(data=df_train, x=df_train['Date'], y=df_train['Qty'])
sns.lineplot(data=df_test, x=df_test['Date'], y=df_test['Qty'])

plt.show()

autocorrelation_plot(df_regresi['Qty']);

"""# Metode Time Series ARIMA"""

# Define the rase function
def rase(y_actual, y_pred):
    # Calculate RMSE
    rmse = mean_squared_error(y_actual, y_pred) ** 0.5
    print(f'RMSE value: {rmse}')

# Define the eval function
def eval(y_actual, y_pred):
    # Evaluate machine learning modeling
    rase(y_actual, y_pred)
    mae = mean_absolute_error(y_actual, y_pred)
    print(f'MAE value: {mae}')

# Set the index for df_train and df_test
df_train = df_train.set_index('Date')
df_test = df_test.set_index('Date')

# Perform ARIMA modeling
y = df_train['Qty']
ARIMAmodel = ARIMA(y, order=(40, 2, 1))
ARIMAmodel = ARIMAmodel.fit()

# Make predictions
y_pred = ARIMAmodel.get_forecast(len(df_test))
y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = ARIMAmodel.predict(start=y_pred_df.index[0], end=y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']

# Evaluate and plot the results
eval(df_test['Qty'], y_pred_out)
plt.figure(figsize=(20, 9))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color='red')
plt.plot(y_pred_out, color='black', label='ARIMA Predictions')
plt.legend()
plt.show()

"""# Clustering"""

df_merge.head()

#identifikasi kolom yang memiliki redundant/corelasi tinggi
df_merge.corr()

# Membuat data baru untuk clustering, yaitu groupby by customerID lalu yang di aggregasi
df_cluster = df_merge.groupby('CustomerID').agg({
    'TransactionID': 'count',
    'Qty': 'sum',
    'TotalAmount': 'sum'
}).reset_index()
df_cluster.head()

data_cluster = df_cluster.drop(columns=['CustomerID'])
data_cluster_normalize = preprocessing.normalize(data_cluster)
data_cluster_normalize

"""# Clustering using KMeans Method"""

# Calculate Z-score for each data point
numeric_cols = ['TransactionID', 'Qty', 'TotalAmount']
z_scores = np.abs((df_cluster[numeric_cols] - df_cluster[numeric_cols].mean()) / df_cluster[numeric_cols].std())

# Define a threshold for outliers
outlier_threshold = 3

# Identify outliers
outliers = df_cluster[z_scores > outlier_threshold]

# Plot the data
plt.figure(figsize=(12, 6))

# Plot for 'Qty'
plt.subplot(1, 3, 1)
plt.scatter(df_cluster.index, df_cluster['Qty'], label='Qty')
plt.scatter(outliers.index, outliers['Qty'], color='red', label='Outliers (Qty)')
plt.xlabel('Index')
plt.ylabel('Qty')
plt.title('Qty')

# Plot for 'TransactionID'
plt.subplot(1, 3, 2)
plt.scatter(df_cluster.index, df_cluster['TransactionID'], label='TransactionID')
plt.scatter(outliers.index, outliers['TransactionID'], color='blue', label='Outliers (TransactionID)')
plt.xlabel('Index')
plt.ylabel('TransactionID')
plt.title('TransactionID')

# Plot for 'TotalAmount'
plt.subplot(1, 3, 3)
plt.scatter(df_cluster.index, df_cluster['TotalAmount'], label='TotalAmount')
plt.scatter(outliers.index, outliers['TotalAmount'], color='green', label='Outliers (TotalAmount)')
plt.xlabel('Index')
plt.ylabel('TotalAmount')
plt.title('TotalAmount')

plt.tight_layout()
plt.show()

"""Tidak diperlukan proses lanjutan karena tidak ada banyak outlier yang signifikan yang dapat terdeteksi."""

# Standardize the Data
# Create a StandardScaler object
scaler = StandardScaler()

# Fit and transform the data in df_cluster
df_cluster_std = scaler.fit_transform(df_cluster.values)

# Convert the standardized data back to a DataFrame
df_cluster_std = pd.DataFrame(df_cluster_std, columns=df_cluster.columns)

# check the df_cluster_std data
df_cluster_std.head()

# check null values
df_cluster_std.isnull().sum()

# plot the elbow method to get best number of clusters
wcss = [] # wcss is Within Cluster of Sum Squares
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init = 10)
    kmeans.fit(df_cluster_std.values)
    wcss.append(kmeans.inertia_)

# plot the elbow method
sns.set()
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method Graphs')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

"""dapat dilihat bahwa clusters Kmeans terbaik adalah di n = 3

**KMeans Model Segmentation**
"""

# make KMeans with n = 3
kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0, n_init = 10)
clusters = kmeans.fit_predict(df_cluster_std)
df_cluster['cluster'] = clusters

# Display the DataFrame with the cluster labels
df_cluster.head()

# Convert 'cluster' column to categorical data type
df_cluster['cluster'] = df_cluster['cluster'].astype('category')

# Create the scatter plot using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Qty', y='TotalAmount', data=df_cluster, hue='cluster', palette='Set1', s=70)
plt.xlabel('Quantity')
plt.ylabel('Total Amount')
plt.title('KMeans Clustering Customer Segmentation')
plt.legend(title='Cluster')
plt.show()

K = range(2, 8)
fits = []
score = []

for k in K:
    model = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(data_cluster_normalize)
    fits.append(model)
    score.append(silhouette_score(data_cluster_normalize, model.labels_, metric='euclidean'))

# Visualisasi silhouette score
sns.lineplot(x = K, y = score);

fits[2]

df_cluster['cluster label'] = fits[2].labels_

df_cluster

# Mengelompokkan DataFrame
df_cluster.groupby(['cluster label']).agg({
    'CustomerID':'count',
    'TransactionID' : 'mean',
    'Qty': 'mean',
    'TotalAmount' : 'mean'
})

plt.figure(figsize=(3,3))
sns.pairplot(data=df_cluster,hue='cluster label',palette='Set2')
plt.show()